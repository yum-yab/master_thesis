\chapter{Methodology}
\label{ch:methodology}

\section{Overview}

Explain what I want to do using the CMIP6 simulations: Describe what the general plan is: Visualisation of the moisture transport in Europe with the help . 
Also define what the goals of the visualisations are: Visualize different scenarios for comparison, visualize uncertainties of different members, visualize evolution over time, also try combining those. 
Here should be a graphic that explains the workflow that transforms a simulation into some nice pictures


\section{Preprocessing}

The goal of this step is to prepare the data for further usage. 
One main goal is to reduce the size, so first of all the geographic region of interest is cut out, which is the Northern Atlantic (derived from \cite{vietinghoff_visual_2021}): $-90^\circ W - 40^\circ E$, $20^\circ - 80^\circ N$. 
For the compared variables (preciptation, surface pressure) the preprocessing is already done, but in case of IVT it still needs to be calculated. 


\begin{enumerate}
  \item Load four different fields for each time period in each member in each szenario: Specific humidity $hus$, eastward horizontal wind $ua$, northward horizontal wind $va$ and surface pressure $ps$. Here the geographical box around the area of interest is cut out: Europe and the North Atlantic (Longitude: $-90 \to 40$, Latitude: $20 \to 80$, based on \cite{vietinghoff_visual_2021})
  \item For each geographical gridpoint ($lon$, $lat$) and timestep: Calculate the integrals of hte product of wind components and specific humidity over the vertical pressure levels $\frac{1}{g}\int_{p_S}^0 hus * ua$ and  $\frac{1}{g}\int_{p_S}^0 hus * va$, with $g$ being the gravitational acceleration ($9.806 \frac{m}{s}$) 
  \item Save the results for each time period in each member in each szenario in a NetCDF file for the further steps. 
  
\end{enumerate}

The calculations were performed on the high performance computing cluster\footnote{https://docs.dkrz.de/doc/levante/} of the German Climate Calculations Center (DKRZ), due to the MPI GE CMIP6 is saved there and downloading the data would take a lot of time. 
This also result in the goal of this step to minimize the hours on the HPC system since they get billed by the time using nodes. 
Although these steps seem easy, due to the large sizes of the datasets and other issues many challenges were met. 
In the following those will be explained with regard to the step they ocurred in. 



\textbf{1. Data Loading}

\begin{itemize}
  \item cutting out geobox
  \item slow IO $\rightarrow$ started using dask+xarray 
\end{itemize}

\textbf{2. Vertical integration}

\begin{itemize}
  \item calculate hybrid  sigma pressure levels for each gridpoint and timestep to get the x values for integration
  \item describe my idea of testing the integration
  
\end{itemize}

\subsection{Problems with Preprocessing}

The steps described in the section before were just the final attempt. 
The first (and many following) coding versions were implemented in Julia \cite{gao_julia_2020}, using just a NetCDF library\footnote{\url{https://alexander-barth.github.io/NCDatasets.jl/stable/}} while the rest was coded from scratch. 
The algorithm was very simple: 

\begin{enumerate}[itemsep=0mm]
  \item Load all datasets into the RAM (as recommended by the NetCDF library itself) and cut out the used gegraphical limits. This should be feasible since all in all one dataset for one timescope-file accounts for \~12 GB\footnote{$70~lon * 32~lat * 47~levels * 29220~timesteps * 4~byte \approx 12 GB$}, so the maximum is around $36~GB$, since the surface pressure data is not that large (\~$260~MB$)   
  \item Calculate the IVT with trapezoidial integration multithreaded by handling one timestep by one thread
  \item Write the results (euklidian norm and the meridional/zonal component)
\end{enumerate}

Although julia promises high performance, it performed quite poorly on the HPC.
The reason for this is the slow IO on the cluster: While the calculation itself took only $\tilde 235~s$ ($\approx 4~min$)\footnote{Refering here and  in the following to one timescope of 20 years in one member}, the loading of the required datasets took around $\tilde 3350~s$ ($\approx 55~min$). 
This results in roughly $5 h$ (including saving the data to disk) for one member of ScenarioMIP, which leads to $250~h$ node hours for one scenario. 
Taking into account that it needs to run for historical simulations as well as other scenarios, this was not feasible according to the limited node hours provided\footnote{Also taking into account that the processes may need to run multiple times due to errors}.

To reduce the loading time of the data multiple optimizations were evaluated. 

\begin{itemize}
  \item reducing the amount of data moved in RAM by preallocating the needed space
  \item 
\end{itemize}
% This should explain how you get a proper IVT field from a CMIP6 simulation, what the caveats are and what calculations are done. 
%
%
%
% \begin{itemize}
%   \item intitial implementation in julia $\rightarrow$ computation was fast, but the slow IO was a problem 
%   \item also very established tools (like CDO) fail 
%   \item multiple ideas of optimising the IO:
%     \begin{enumerate}
%       \item Multiple different libraries - similar results
%       \item Parallel reading of different fields - no success or segmentation fault
%     \end{enumerate}
%     
%   \item after asking the helpdesk $\rightarrow$ problem was discovered (few byte transmissions) but not the reason for it 
%   \item helpdesk says that large portions of the datasets were never used
%   \item after investigating: Turns out Pythons xarray+dask was quite performant
% \end{itemize}
%
